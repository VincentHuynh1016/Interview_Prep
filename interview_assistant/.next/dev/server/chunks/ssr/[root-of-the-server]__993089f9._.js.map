{
  "version": 3,
  "sources": [],
  "sections": [
    {"offset": {"line": 10, "column": 0}, "map": {"version":3,"sources":["file:///Users/vincenthuynh/Desktop/Interview_Helper/interview_assistant/app/interview/page.tsx"],"sourcesContent":["\"use client\";\nimport { useEffect, useRef, useState } from \"react\";\n\nexport default function InterviewPage() {\n  //Access to video element\n  const videoRef = useRef<HTMLVideoElement>(null);\n  //Grabs a list of interview questions\n  const [questions, setQuestions] = useState<any[]>([]);\n  //Holds the chat messages\n  const [messages, setMessages] = useState<any[]>([]);\n  //Access to chat container for scrolling\n  const chatRef = useRef<HTMLDivElement | null>(null);\n  //To check if the audio is unlocked\n  const [audioUnlocked, setAudioUnlocked] = useState(false);\n  //Used to access the questions array\n  const [qIndex, setQIndex] = useState(0);\n  //To check if the interview has started\n  const [started, setStarted] = useState(false);\n\n  //Unlocks audio on first user interaction\n  function unlockAudioOnce() {\n    if (!audioUnlocked) {\n      window.speechSynthesis.speak(new SpeechSynthesisUtterance(\"\"));\n      setAudioUnlocked(true);\n    }\n  }\n\n  function startInterview() {\n    unlockAudioOnce(); // browser permission\n    setStarted(true); // switch panel → chat\n  }\n\n  //This is for accessing the speech recognition\n  function startListening() {\n    //Creates a new instance of speech recognition\n    const SpeechRecognition =\n      (window as any).SpeechRecognition ||\n      (window as any).webkitSpeechRecognition;\n\n    if (!SpeechRecognition) {\n      console.warn(\"SpeechRecognition is not supported in this browser.\");\n      return;\n    }\n\n    const recognition = new SpeechRecognition();\n    recognition.lang = \"en-US\";\n    recognition.start();\n\n    recognition.onresult = (e: SpeechRecognitionEvent) => {\n      const transcript = e.results[0][0].transcript;\n      recognition.stop();\n      setMessages((prev) => [...prev, { role: \"user\", text: transcript }]);\n      setQIndex((i) => i + 1);\n    };\n  }\n\n  // This if for accessing the speech synthesis\n  function startSpeaking(text: string) {\n    const synth = window.speechSynthesis;\n    synth.cancel();\n\n    //Get the most recent\n    const utter = new SpeechSynthesisUtterance(text);\n    synth.speak(utter);\n  }\n\n  useEffect(() => {\n    if (!questions[qIndex]) return;\n    const txt = questions[qIndex].text;\n\n    setMessages((prev) => {\n      const last = prev[prev.length - 1];\n      if (last?.role === \"bot\" && last?.text === txt) return prev;\n      return [...prev, { role: \"bot\", text: txt }];\n    });\n\n    if (!audioUnlocked) return;\n    startSpeaking(txt);\n  }, [questions, qIndex, audioUnlocked]);\n\n  //Scroll to bottom of chat when new message is added\n  useEffect(() => {\n    if (chatRef.current) {\n      chatRef.current.scrollTop = chatRef.current.scrollHeight;\n    }\n  }, [messages]);\n\n  // Fetch the interview questions from local JSON file (TEMPORARY)\n  useEffect(() => {\n    fetch(\"/questions.json\")\n      .then((res) => res.json())\n      .then((data) => setQuestions(data.questions));\n  }, []);\n\n  // Permission to access webcam if so then stream video to video element\n  useEffect(() => {\n    navigator.mediaDevices.getUserMedia({ video: true }).then((stream) => {\n      if (videoRef.current) {\n        videoRef.current.srcObject = stream;\n      }\n    });\n  }, []);\n\n  return (\n    <div\n      className=\"flex h-screen p-6 gap-4\"\n      style={{ backgroundColor: \"rgb(43, 48, 59)\" }}\n    >\n      <video\n        ref={videoRef}\n        autoPlay\n        className=\"w-[92%] h-[90%] rounded-xl object-cover\"\n      />\n\n      {!started ? (\n        <div className=\"w-[30%] h-[90%] flex flex-col justify-center items-center bg-white rounded-xl text-black\">\n          <p className=\"text-2xl mb-4\">Ready to join?</p>\n          <button\n            className=\"bg-brand bg-blue-700 text-white box-border border border-transparent hover:bg-brand-strong focus:ring-4 focus:ring-brand-medium shadow-xs font-medium leading-5 rounded-lg text-sm px-4 py-2.5 focus:outline-none mt-3\"\n            onClick={startInterview}\n          >\n            Join call\n          </button>\n        </div>\n      ) : (\n        <div\n          ref={chatRef}\n          className=\"w-[30%] h-[90%] rounded-xl bg-white p-4 mb-4 overflow-y-auto\"\n        >\n          {messages.map((m, i) => (\n            <div\n              key={i}\n              className={`flex mb-3 ${\n                m.role === \"bot\" ? \"justify-start\" : \"justify-end\"\n              }`}\n            >\n              <p\n                className={\n                  m.role === \"bot\"\n                    ? \"bg-cyan-200 text-black rounded-full px-4 py-2\"\n                    : \"bg-emerald-400 text-black rounded-full px-4 py-2\"\n                }\n              >\n                {m.text}\n              </p>\n            </div>\n          ))}\n          <button className=\"text-black\" onClick={startListening}>\n            Speak\n          </button>\n        </div>\n      )}\n    </div>\n  );\n}\n\n// NEXT STEP:\n// What you *can* do instead:\n\n// * **Show the first question as text immediately**, and show a big **“Start interview (enables audio)”** button.\n// * Or **auto-start after any user gesture** (first click anywhere / keypress) by attaching `unlockAudioOnce` to the container.\n// * Or start audio only after they click your existing **Speak** button (same idea).\n\n// If you want it to feel automatic, the “click anywhere to start” pattern is the closest you’ll get while staying consistent across browsers.\n"],"names":[],"mappings":";;;;;AACA;AADA;;;AAGe,SAAS;IACtB,yBAAyB;IACzB,MAAM,WAAW,IAAA,+MAAM,EAAmB;IAC1C,qCAAqC;IACrC,MAAM,CAAC,WAAW,aAAa,GAAG,IAAA,iNAAQ,EAAQ,EAAE;IACpD,yBAAyB;IACzB,MAAM,CAAC,UAAU,YAAY,GAAG,IAAA,iNAAQ,EAAQ,EAAE;IAClD,wCAAwC;IACxC,MAAM,UAAU,IAAA,+MAAM,EAAwB;IAC9C,mCAAmC;IACnC,MAAM,CAAC,eAAe,iBAAiB,GAAG,IAAA,iNAAQ,EAAC;IACnD,oCAAoC;IACpC,MAAM,CAAC,QAAQ,UAAU,GAAG,IAAA,iNAAQ,EAAC;IACrC,uCAAuC;IACvC,MAAM,CAAC,SAAS,WAAW,GAAG,IAAA,iNAAQ,EAAC;IAEvC,yCAAyC;IACzC,SAAS;QACP,IAAI,CAAC,eAAe;YAClB,OAAO,eAAe,CAAC,KAAK,CAAC,IAAI,yBAAyB;YAC1D,iBAAiB;QACnB;IACF;IAEA,SAAS;QACP,mBAAmB,qBAAqB;QACxC,WAAW,OAAO,sBAAsB;IAC1C;IAEA,8CAA8C;IAC9C,SAAS;QACP,8CAA8C;QAC9C,MAAM,oBACJ,AAAC,OAAe,iBAAiB,IACjC,AAAC,OAAe,uBAAuB;QAEzC,IAAI,CAAC,mBAAmB;YACtB,QAAQ,IAAI,CAAC;YACb;QACF;QAEA,MAAM,cAAc,IAAI;QACxB,YAAY,IAAI,GAAG;QACnB,YAAY,KAAK;QAEjB,YAAY,QAAQ,GAAG,CAAC;YACtB,MAAM,aAAa,EAAE,OAAO,CAAC,EAAE,CAAC,EAAE,CAAC,UAAU;YAC7C,YAAY,IAAI;YAChB,YAAY,CAAC,OAAS;uBAAI;oBAAM;wBAAE,MAAM;wBAAQ,MAAM;oBAAW;iBAAE;YACnE,UAAU,CAAC,IAAM,IAAI;QACvB;IACF;IAEA,6CAA6C;IAC7C,SAAS,cAAc,IAAY;QACjC,MAAM,QAAQ,OAAO,eAAe;QACpC,MAAM,MAAM;QAEZ,qBAAqB;QACrB,MAAM,QAAQ,IAAI,yBAAyB;QAC3C,MAAM,KAAK,CAAC;IACd;IAEA,IAAA,kNAAS,EAAC;QACR,IAAI,CAAC,SAAS,CAAC,OAAO,EAAE;QACxB,MAAM,MAAM,SAAS,CAAC,OAAO,CAAC,IAAI;QAElC,YAAY,CAAC;YACX,MAAM,OAAO,IAAI,CAAC,KAAK,MAAM,GAAG,EAAE;YAClC,IAAI,MAAM,SAAS,SAAS,MAAM,SAAS,KAAK,OAAO;YACvD,OAAO;mBAAI;gBAAM;oBAAE,MAAM;oBAAO,MAAM;gBAAI;aAAE;QAC9C;QAEA,IAAI,CAAC,eAAe;QACpB,cAAc;IAChB,GAAG;QAAC;QAAW;QAAQ;KAAc;IAErC,oDAAoD;IACpD,IAAA,kNAAS,EAAC;QACR,IAAI,QAAQ,OAAO,EAAE;YACnB,QAAQ,OAAO,CAAC,SAAS,GAAG,QAAQ,OAAO,CAAC,YAAY;QAC1D;IACF,GAAG;QAAC;KAAS;IAEb,iEAAiE;IACjE,IAAA,kNAAS,EAAC;QACR,MAAM,mBACH,IAAI,CAAC,CAAC,MAAQ,IAAI,IAAI,IACtB,IAAI,CAAC,CAAC,OAAS,aAAa,KAAK,SAAS;IAC/C,GAAG,EAAE;IAEL,uEAAuE;IACvE,IAAA,kNAAS,EAAC;QACR,UAAU,YAAY,CAAC,YAAY,CAAC;YAAE,OAAO;QAAK,GAAG,IAAI,CAAC,CAAC;YACzD,IAAI,SAAS,OAAO,EAAE;gBACpB,SAAS,OAAO,CAAC,SAAS,GAAG;YAC/B;QACF;IACF,GAAG,EAAE;IAEL,qBACE,8OAAC;QACC,WAAU;QACV,OAAO;YAAE,iBAAiB;QAAkB;;0BAE5C,8OAAC;gBACC,KAAK;gBACL,QAAQ;gBACR,WAAU;;;;;;YAGX,CAAC,wBACA,8OAAC;gBAAI,WAAU;;kCACb,8OAAC;wBAAE,WAAU;kCAAgB;;;;;;kCAC7B,8OAAC;wBACC,WAAU;wBACV,SAAS;kCACV;;;;;;;;;;;qCAKH,8OAAC;gBACC,KAAK;gBACL,WAAU;;oBAET,SAAS,GAAG,CAAC,CAAC,GAAG,kBAChB,8OAAC;4BAEC,WAAW,CAAC,UAAU,EACpB,EAAE,IAAI,KAAK,QAAQ,kBAAkB,eACrC;sCAEF,cAAA,8OAAC;gCACC,WACE,EAAE,IAAI,KAAK,QACP,kDACA;0CAGL,EAAE,IAAI;;;;;;2BAZJ;;;;;kCAgBT,8OAAC;wBAAO,WAAU;wBAAa,SAAS;kCAAgB;;;;;;;;;;;;;;;;;;AAOlE,EAEA,aAAa;CACb,6BAA6B;CAE7B,kHAAkH;CAClH,gIAAgI;CAChI,qFAAqF;CAErF,8IAA8I"}},
    {"offset": {"line": 210, "column": 0}, "map": {"version":3,"sources":["file:///Users/vincenthuynh/Desktop/Interview_Helper/interview_assistant/node_modules/next/src/server/route-modules/app-page/module.compiled.js"],"sourcesContent":["if (process.env.NEXT_RUNTIME === 'edge') {\n  module.exports = require('next/dist/server/route-modules/app-page/module.js')\n} else {\n  if (process.env.__NEXT_EXPERIMENTAL_REACT) {\n    if (process.env.NODE_ENV === 'development') {\n      if (process.env.TURBOPACK) {\n        module.exports = require('next/dist/compiled/next-server/app-page-turbo-experimental.runtime.dev.js')\n      } else {\n        module.exports = require('next/dist/compiled/next-server/app-page-experimental.runtime.dev.js')\n      }\n    } else {\n      if (process.env.TURBOPACK) {\n        module.exports = require('next/dist/compiled/next-server/app-page-turbo-experimental.runtime.prod.js')\n      } else {\n        module.exports = require('next/dist/compiled/next-server/app-page-experimental.runtime.prod.js')\n      }\n    }\n  } else {\n    if (process.env.NODE_ENV === 'development') {\n      if (process.env.TURBOPACK) {\n        module.exports = require('next/dist/compiled/next-server/app-page-turbo.runtime.dev.js')\n      } else {\n        module.exports = require('next/dist/compiled/next-server/app-page.runtime.dev.js')\n      }\n    } else {\n      if (process.env.TURBOPACK) {\n        module.exports = require('next/dist/compiled/next-server/app-page-turbo.runtime.prod.js')\n      } else {\n        module.exports = require('next/dist/compiled/next-server/app-page.runtime.prod.js')\n      }\n    }\n  }\n}\n"],"names":["process","env","NEXT_RUNTIME","module","exports","require","__NEXT_EXPERIMENTAL_REACT","NODE_ENV","TURBOPACK"],"mappings":"AAAA,IAAIA,QAAQC,GAAG,CAACC,YAAY,KAAK,QAAQ;;KAElC;IACL,IAAIF,QAAQC,GAAG,CAACK,yBAAyB,EAAE;;SAcpC;QACL,IAAIN,QAAQC,GAAG,CAACM,QAAQ,KAAK,WAAe;YAC1C,IAAIP,QAAQC,GAAG,CAACO,SAAS,eAAE;gBACzBL,OAAOC,OAAO,GAAGC,QAAQ;YAC3B,OAAO;;QAGT,OAAO;;IAOT;AACF","ignoreList":[0]}},
    {"offset": {"line": 229, "column": 0}, "map": {"version":3,"sources":["file:///Users/vincenthuynh/Desktop/Interview_Helper/interview_assistant/node_modules/next/src/server/route-modules/app-page/vendored/ssr/react-jsx-dev-runtime.ts"],"sourcesContent":["module.exports = (\n  require('../../module.compiled') as typeof import('../../module.compiled')\n).vendored['react-ssr']!.ReactJsxDevRuntime\n"],"names":["module","exports","require","vendored","ReactJsxDevRuntime"],"mappings":"AAAAA,OAAOC,OAAO,GACZC,QAAQ,4HACRC,QAAQ,CAAC,YAAY,CAAEC,kBAAkB","ignoreList":[0]}},
    {"offset": {"line": 234, "column": 0}, "map": {"version":3,"sources":["file:///Users/vincenthuynh/Desktop/Interview_Helper/interview_assistant/node_modules/next/src/server/route-modules/app-page/vendored/ssr/react.ts"],"sourcesContent":["module.exports = (\n  require('../../module.compiled') as typeof import('../../module.compiled')\n).vendored['react-ssr']!.React\n"],"names":["module","exports","require","vendored","React"],"mappings":"AAAAA,OAAOC,OAAO,GACZC,QAAQ,4HACRC,QAAQ,CAAC,YAAY,CAAEC,KAAK","ignoreList":[0]}}]
}